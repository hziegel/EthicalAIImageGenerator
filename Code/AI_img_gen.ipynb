{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqlDq3uYobw04KTGe2SJbM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ethical AI Image Generator using Variational Autoencoder"],"metadata":{"id":"Wm2aYK0GvANQ"}},{"cell_type":"markdown","source":["Connect to Google Drive\n","\n","* Run first every time if storing images in Google Drive"],"metadata":{"id":"LFmhzBL83eG3"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"oVLR3NNPi_lY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761516599621,"user_tz":420,"elapsed":20097,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}},"outputId":"f4dbf1e8-dd20-41d1-b7c9-0b1498253bc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"lostwcnc59xX"}},{"cell_type":"code","source":["from PIL import Image\n","from pathlib import Path\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"],"metadata":{"id":"W491nkdh50s2","executionInfo":{"status":"ok","timestamp":1761516615196,"user_tz":420,"elapsed":14096,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Configuration Setup\n","* Change file path to run locally"],"metadata":{"id":"m3NoOEQg4rLx"}},{"cell_type":"code","source":["CONFIG = {\n","    'drive_folder': '/content/drive/MyDrive/Ethical_AI_Image/Images/Training',  # Change this path\n","    'image_size': 256,  # Resolution of images\n","    'latent_dim': 256,  # Size of latent space\n","    'batch_size': 16,\n","    'epochs': 50,\n","    'learning_rate': 1e-4,\n","    'beta': 1.0,  # Weight for KL divergence loss\n","}"],"metadata":{"id":"pIDlX0Ig30mZ","executionInfo":{"status":"ok","timestamp":1761516575371,"user_tz":420,"elapsed":7,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Dataset Class for Dataset"],"metadata":{"id":"F8Z0ojq8-V0-"}},{"cell_type":"code","source":["class ArtDataset(Dataset):\n","    \"\"\"Dataset for loading donated art images\"\"\"\n","    def __init__(self, folder_path, image_size=256):\n","        self.folder_path = folder_path\n","        self.image_size = image_size\n","\n","        # Get all image files\n","        self.image_paths = []\n","        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n","\n","        for ext in valid_extensions:\n","            self.image_paths.extend(Path(folder_path).glob(f'**/*{ext}'))\n","            self.image_paths.extend(Path(folder_path).glob(f'**/*{ext.upper()}'))\n","\n","        self.image_paths = [str(p) for p in self.image_paths]\n","        print(f\"Found {len(self.image_paths)} images in {folder_path}\")\n","\n","        # Image transformations\n","        self.transform = transforms.Compose([\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        image = self.transform(image)\n","        return image"],"metadata":{"id":"SfD6Ppvo5IyK","executionInfo":{"status":"ok","timestamp":1761516618013,"user_tz":420,"elapsed":16,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Define Class for Variational Autoencoder"],"metadata":{"id":"l3LK3E-bAXFX"}},{"cell_type":"code","source":["class VAE(nn.Module):\n","    \"\"\"Variational Autoencoder for image generation\"\"\"\n","    def __init__(self, latent_dim=256):\n","        super(VAE, self).__init__()\n","        self.latent_dim = latent_dim\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","          nn.Conv2d(3, 32, 4, stride=2, padding=1),    # 256 -> 128\n","          nn.ReLU(),\n","          nn.Conv2d(32, 32, 4, stride=2, padding=1),   # 128 -> 64\n","          nn.ReLU(),\n","          nn.Conv2d(32, 64, 4, stride=2, padding=1),   # 64 -> 32\n","          nn.ReLU(),\n","          nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 32 -> 16\n","          nn.ReLU(),\n","          nn.Conv2d(128, 256, 4, stride=2, padding=1), # 16 -> 8\n","          nn.ReLU(),\n","          nn.Conv2d(256, 512, 4, stride=2, padding=1), # 8 -> 4\n","          nn.ReLU(),\n","        )\n","\n","        # Latent space\n","        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n","        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n","\n","        # Decoder input\n","        self.decoder_input = nn.Linear(latent_dim, 512 * 4 * 4)\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","          nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # 4 -> 8\n","          nn.ReLU(),\n","          nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 8 -> 16\n","          nn.ReLU(),\n","          nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),   # 16 -> 32\n","          nn.ReLU(),\n","          nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),    # 32 -> 64\n","          nn.ReLU(),\n","          nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),    # 64 -> 128\n","          nn.ReLU(),\n","          nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),     # 128 -> 256\n","          nn.Tanh()\n","        )\n","\n","    def encode(self, x):\n","        x = self.encoder(x)\n","        x = x.view(x.size(0), -1)\n","        mu = self.fc_mu(x)\n","        logvar = self.fc_logvar(x)\n","        return mu, logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def decode(self, z):\n","        x = self.decoder_input(z)\n","        x = x.view(x.size(0), 512, 4, 4)\n","        x = self.decoder(x)\n","        return x\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        recon = self.decode(z)\n","        return recon, mu, logvar"],"metadata":{"id":"cV1r2PN1-Z2b","executionInfo":{"status":"ok","timestamp":1761518498951,"user_tz":420,"elapsed":10,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["Loss Function"],"metadata":{"id":"EuNyDaPSAr9i"}},{"cell_type":"code","source":["def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n","    \"\"\"VAE loss = reconstruction loss + KL divergence\"\"\"\n","    # Reconstruction loss (MSE)\n","    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n","\n","    # KL divergence loss\n","    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","\n","    return recon_loss + beta * kl_loss"],"metadata":{"id":"jD3M3032Afwl","executionInfo":{"status":"ok","timestamp":1761516623917,"user_tz":420,"elapsed":5,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Training Function"],"metadata":{"id":"9uloxbB6A2NR"}},{"cell_type":"code","source":["def train_vae(model, dataloader, optimizer, device, beta=1.0):\n","    \"\"\"Train VAE for one epoch\"\"\"\n","    model.train()\n","    total_loss = 0\n","\n","    for batch_idx, data in enumerate(dataloader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","\n","        recon_batch, mu, logvar = model(data)\n","        loss = vae_loss(recon_batch, data, mu, logvar, beta)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item()/len(data):.4f}')\n","\n","    return total_loss / len(dataloader.dataset)"],"metadata":{"id":"fMCXJm8hAwVc","executionInfo":{"status":"ok","timestamp":1761516625818,"user_tz":420,"elapsed":4,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Function to Generate Output"],"metadata":{"id":"f2P4MLrdBGVB"}},{"cell_type":"code","source":["def generate_artwork(model, device, num_samples=1):\n","    \"\"\"Generate new artwork from the trained VAE\"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        # Sample from standard normal distribution\n","        z = torch.randn(num_samples, model.latent_dim).to(device)\n","        samples = model.decode(z)\n","        samples = samples.cpu()\n","    return samples"],"metadata":{"id":"ulofIQ0NBA5l","executionInfo":{"status":"ok","timestamp":1761516628163,"user_tz":420,"elapsed":3,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Function to Save Output"],"metadata":{"id":"Zb7txJoKBc1t"}},{"cell_type":"code","source":["def save_image(tensor, filepath):\n","    \"\"\"Save a tensor as an image\"\"\"\n","    # Denormalize from [-1, 1] to [0, 1]\n","    tensor = (tensor + 1) / 2\n","    tensor = torch.clamp(tensor, 0, 1)\n","\n","    # Convert to PIL Image\n","    img = transforms.ToPILImage()(tensor)\n","    img.save(filepath)\n","    print(f\"Saved image to {filepath}\")"],"metadata":{"id":"KeiL3Jj_BOBL","executionInfo":{"status":"ok","timestamp":1761516630483,"user_tz":420,"elapsed":9,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Function to Visualize How it Works"],"metadata":{"id":"GfBuomD9BmXG"}},{"cell_type":"code","source":["def visualize_results(original, reconstructed, generated):\n","    \"\"\"Visualize original, reconstructed, and generated images\"\"\"\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","    # Denormalize images\n","    for img in [original, reconstructed, generated]:\n","        img.data = (img.data + 1) / 2\n","        img.data = torch.clamp(img.data, 0, 1)\n","\n","    axes[0].imshow(original.permute(1, 2, 0).cpu())\n","    axes[0].set_title('Original')\n","    axes[0].axis('off')\n","\n","    axes[1].imshow(reconstructed.permute(1, 2, 0).cpu())\n","    axes[1].set_title('Reconstructed')\n","    axes[1].axis('off')\n","\n","    axes[2].imshow(generated.permute(1, 2, 0).cpu())\n","    axes[2].set_title('Generated')\n","    axes[2].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"Le59TLGoBhlz","executionInfo":{"status":"ok","timestamp":1761516635709,"user_tz":420,"elapsed":42,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Training Pipeline"],"metadata":{"id":"7vacNUh3CDTX"}},{"cell_type":"code","source":["def main():\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Load dataset\n","    print(\"\\nLoading dataset...\")\n","    dataset = ArtDataset(CONFIG['drive_folder'], CONFIG['image_size'])\n","\n","    if len(dataset) == 0:\n","        print(\"ERROR: No images found! Please check your folder path.\")\n","        print(f\"Looking in: {CONFIG['drive_folder']}\")\n","        return\n","\n","    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'],\n","                          shuffle=True, num_workers=2)\n","\n","    # Initialize model\n","    print(\"\\nInitializing VAE model...\")\n","    model = VAE(latent_dim=CONFIG['latent_dim']).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n","\n","    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","    # Training loop\n","    print(\"\\nStarting training...\")\n","    losses = []\n","\n","    for epoch in range(CONFIG['epochs']):\n","        print(f\"\\n{'='*60}\")\n","        print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n","        print('='*60)\n","\n","        avg_loss = train_vae(model, dataloader, optimizer, device, CONFIG['beta'])\n","        losses.append(avg_loss)\n","        print(f\"Average Loss: {avg_loss:.4f}\")\n","\n","        # Generate and save artwork every 10 epochs\n","        if (epoch + 1) % 10 == 0 or epoch == CONFIG['epochs'] - 1:\n","            print(\"\\nGenerating new artwork...\")\n","            generated = generate_artwork(model, device, num_samples=1)[0]\n","\n","            # Save the generated artwork\n","            output_path = f'/content/drive/MyDrive/Ethical_AI_Image/Images/Output/Temp/generated_artwork_epoch_{epoch+1}.png'\n","            save_image(generated, output_path)\n","\n","            # Visualize results\n","            sample_data = next(iter(dataloader))[0:1].to(device)\n","            with torch.no_grad():\n","                recon, _, _ = model(sample_data)\n","            visualize_results(sample_data[0], recon[0], generated)\n","\n","    # Plot training loss\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(losses)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('VAE Training Loss')\n","    plt.grid(True)\n","    plt.show()\n","\n","    # Save final model\n","    model_path = '/content/drive/MyDrive/Ethical_AI_Image/Code/Model/vae_ethical_art_model.pth'\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"\\nModel saved to {model_path}\")\n","\n","    # Generate final artwork\n","    print(\"\\nGenerating final artwork...\")\n","    final_artwork = generate_artwork(model, device, num_samples=1)[0]\n","    final_path = '/content/drive/MyDrive/Ethical_AI_Image/Images/Output/Final/final_generated_artwork.png'\n","    save_image(final_artwork, final_path)\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Training complete\")\n","    print(\"=\"*60)"],"metadata":{"id":"mAVd9_yvB4kR","executionInfo":{"status":"ok","timestamp":1761516639667,"user_tz":420,"elapsed":16,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Train Model"],"metadata":{"id":"SakXJeHMEjuL"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1mh3nvheEPS94jMyA6HKrcIdywO9BjlQ4"},"id":"zljVs7dgEiYS","executionInfo":{"status":"ok","timestamp":1761518541928,"user_tz":420,"elapsed":38583,"user":{"displayName":"Heron Ziegel","userId":"13942346757897084049"}},"outputId":"9abdc35e-1b92-4f85-c7bb-8a381bd2d7c0"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}